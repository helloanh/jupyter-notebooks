{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices Prediction with Llinear Regression and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#importing-dataset-using-panda\" data-toc-modified-id=\"importing-dataset-using-panda-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>importing dataset using panda</a></span></li><li><span><a href=\"#EDA-&amp;-Data-Preprocessing:\" data-toc-modified-id=\"EDA-&amp;-Data-Preprocessing:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>EDA &amp; Data Preprocessing:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-for-Missing-Values\" data-toc-modified-id=\"Checking-for-Missing-Values-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Checking for Missing Values</a></span></li><li><span><a href=\"#Checking-for-Categorical-Data\" data-toc-modified-id=\"Checking-for-Categorical-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Checking for Categorical Data</a></span></li><li><span><a href=\"#Dropping-Columns\" data-toc-modified-id=\"Dropping-Columns-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Dropping Columns</a></span></li><li><span><a href=\"#Understanding-Data-Distribution\" data-toc-modified-id=\"Understanding-Data-Distribution-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Understanding Data Distribution</a></span></li><li><span><a href=\"#Correlation-Between-Columns\" data-toc-modified-id=\"Correlation-Between-Columns-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Correlation Between Columns</a></span></li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Outliers</a></span></li><li><span><a href=\"#Normalizing-Data\" data-toc-modified-id=\"Normalizing-Data-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Normalizing Data</a></span></li></ul></li><li><span><a href=\"#Multiple-Linear-Regression\" data-toc-modified-id=\"Multiple-Linear-Regression-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Multiple Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Separating-Independent-and-Dependent-Variable\" data-toc-modified-id=\"Separating-Independent-and-Dependent-Variable-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Separating Independent and Dependent Variable</a></span></li><li><span><a href=\"#Splitting-Dataset-into-Training-and-Testing-Dataset\" data-toc-modified-id=\"Splitting-Dataset-into-Training-and-Testing-Dataset-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Splitting Dataset into Training and Testing Dataset</a></span></li><li><span><a href=\"#Fit-Linear-Regression-Model\" data-toc-modified-id=\"Fit-Linear-Regression-Model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Fit Linear Regression Model</a></span></li><li><span><a href=\"#Predicting-the-Test-set-Results\" data-toc-modified-id=\"Predicting-the-Test-set-Results-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Predicting the Test set Results</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Model Evaluation</a></span></li><li><span><a href=\"#Predictions-from-our-Model\" data-toc-modified-id=\"Predictions-from-our-Model-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Predictions from our Model</a></span></li><li><span><a href=\"#Residual-Histogram\" data-toc-modified-id=\"Residual-Histogram-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Residual Histogram</a></span></li><li><span><a href=\"#Regression-Evaluation-Metrics\" data-toc-modified-id=\"Regression-Evaluation-Metrics-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Regression Evaluation Metrics</a></span></li></ul></li><li><span><a href=\"#Backward-Elimination\" data-toc-modified-id=\"Backward-Elimination-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Backward Elimination</a></span></li><li><span><a href=\"#Ridge-and-LASSO-Regression\" data-toc-modified-id=\"Ridge-and-LASSO-Regression-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Ridge and LASSO Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ridge-Regression-(L2-Regularization)\" data-toc-modified-id=\"Ridge-Regression-(L2-Regularization)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Ridge Regression (L2-Regularization)</a></span></li><li><span><a href=\"#Lasso-Regression-(L1-Regularization)\" data-toc-modified-id=\"Lasso-Regression-(L1-Regularization)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Lasso Regression (L1-Regularization)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the required libraries this way:\n",
    "#!pip install <library>\n",
    "\n",
    "#uncomment if needed:\n",
    "#!pip install statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing dataset using panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('house_data.csv')\n",
    "#to see what my dataset is comprised of\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any missing values\n",
    "dataset.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "f8388dc1e4c440fe86b8b722c806edffd38fd712",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ade104561cb8dfd2635426ef24cb354421aaea8"
   },
   "outputs": [],
   "source": [
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8bb98eff5e1bd443d7669886adb1649fd21308d9"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['id','date'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some simple plots to check out the data!\n",
    "Seaborn Paiplot will automatically create not just histograms of all the columns but also correllation scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7fffd991f413e2091624663c39111623f52435f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distributions Map no. bedrooms\n",
    "with sns.plotting_context(\"notebook\",font_scale=2.5):\n",
    "    g = sns.pairplot(dataset[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], \n",
    "                 hue='bedrooms', palette='tab20',height=6)\n",
    "g.set(xticklabels=[]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Between Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#heat map of the correlation matrix between each of the columns\n",
    "sns.heatmap(dataset.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotating the correlation values\n",
    "sns.heatmap(dataset.corr(), linewidths=.1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's pick only 5 features/variables/columns\n",
    "sns.heatmap(dataset[['sqft_lot','sqft_above','price','sqft_living','bedrooms']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the figure size\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "sns.heatmap(dataset[['sqft_lot','sqft_above','price','sqft_living','bedrooms']].corr(), annot=True, linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must be careful when choosing to drop outliers of the risk of losing valuable information, but we see here in the plot below 2 clear outliers toward the bottom right of the plot representing \"bad\" deals for sellers (low price for large area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (6.0, 6.0) # define size of figure\n",
    "sns.scatterplot(x='sqft_living', y='price', data=dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the distribution of our target feature, we quickly notice that the distribution appears to be righlty skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(dataset[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the skew function from scipy.stats to determine the \"skewness\" of the Price Feature better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# plot histogram of \"SalePrice\"\n",
    "rcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\n",
    "g = sns.distplot(dataset[\"price\"], label=\"Skewness: %.2f\"%(dataset[\"price\"].skew()))\n",
    "g = g.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, our regression models will perform best with normally distributed data. Thus for best results, let's attempt to normalize the feature with a log transform. (For rightly skewed data, a log transform has the effect of shifting the distribution to appear more \"normal\", while for leftly skewed data, a log transform will only make the distribution even more leftly skewed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedSalePrice = np.log1p(dataset[\"price\"])\n",
    "\n",
    "# plot histogram of log transformed \"price\"\n",
    "rcParams['figure.figsize'] = (12.0, 6.0) # define size of figure\n",
    "g = sns.distplot(normalizedSalePrice, label=\"Skewness: %.2f\"%(normalizedSalePrice.skew()))\n",
    "g = g.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see our log transform did surprisingly well, and had the intended effect - the new distribution looks much more \"normal\". Let's go ahead and apply this log transformation of \"SalePrice\" to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transform to target\n",
    "dataset[\"price\"] = np.log1p(dataset[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'll see, several of the non-target numerical features are also heavily skewed, both rightly and leftly. For each of these, this time we'll choose to use a blanket \"yeo-johnson\" power transform to attempt to \"normalize\" each of them, since this tranform \"normalizes\" both righlty and leftly skewed data. (Here we consider all features with a \"skewness\" magnitude above 0.75 as \"heavily\" skewed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine features that are heavily skewed\n",
    "def get_skewed_features():\n",
    "    numeric_feats = dataset.dtypes[dataset.dtypes != \"object\"].index\n",
    "    skewed_feats = dataset[numeric_feats].apply(lambda x: skew(x.dropna())) # computes \"skewness\"\n",
    "    skewed_feats = skewed_feats[abs(skewed_feats) > 0.75]\n",
    "    return skewed_feats.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform\n",
    "\n",
    "# find heavily skewed numerical features\n",
    "skewed_feats = get_skewed_features()\n",
    "print(\"{} heavily skewed features.\".format(len(skewed_feats)))\n",
    "\n",
    "# apply power transform to all heavily skewed numeric features\n",
    "dataset[skewed_feats] = power_transform(dataset[skewed_feats], method='yeo-johnson')\n",
    "print(\"Applied power transform.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Why are we \"normalizing\" the numerical features?**\n",
    "\n",
    "In general, standardized or normally distributed data is nice to have, and provides various benefits in different situations. All the specific benefits and situations goes beyond the scope of this notebook, but typically normalizing your data is a good idea in the absence of any other information against the case. In our situation where we plan to use regularization methods, the more extreme observation values in the highly skewed features create a bias that can cause different explanatory variables to be treated not so equally by the regularization penalty term. By normalizing these skewed distributions, it is believed the regularization penalty will then treat different explanatory variables on a more equal footing. Ideally, we want all observations and variables to be treated perfectly equally by our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typcial Ordinary Least Squares Linear Regression model aims to optimize the residual sum of squares (RSS), which is defined as:\n",
    "\n",
    "![Residual Sum of Squares](https://wikimedia.org/api/rest_v1/media/math/render/svg/2f6526aa487b4dc460792bf1eeee79b2bba77709)\n",
    "\n",
    "To analyze how well this model performs for this data set, we will fit the model using the training data, and then estimate the model's average root mean square error (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Independent and Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "757472ac9cd812af4c1cae7d804999265065d2ab"
   },
   "outputs": [],
   "source": [
    "#features:\n",
    "X = dataset.iloc[:,1:]\n",
    "\n",
    "#if need to be changed to array:\n",
    "#X = dataset.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable that we try to predict:\n",
    "y = dataset.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Training and Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0) #you could have: test_size = 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89d1414c6ee7ba113209ca80258ec645644323f8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the intercept\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing you can check out are the coefficients that are related to each feature in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#let's create dataframe from these coefficients -> pd.DataFrame(data, index, call index name)\n",
    "coeff_df = pd.DataFrame(regressor.coef_ , X.columns,columns=['Coefficient'])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Discussion Question 1) Choose 5 Coefficients from the above table and Interpret them.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get largest magnitude coefficients\n",
    "coef_1 = pd.Series(regressor.coef_, index = X_train.columns)\n",
    "imp_coef = pd.concat([coef_1.sort_values().head(10), coef_1.sort_values().tail(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (8.0, 10.0) # define size of figure\n",
    "coef_1.plot(kind = \"barh\")\n",
    "plt.title(\"Most Important Coefficients Selected by Linear Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from non-relevant \"lat\" which is the latitude, we don't see any really high coefficient values chosen here because we did a fairly good job preprocessing our data. Had we not removed outliers and normalized the skewed numerical features for example, there would have been higher variance and a high chance of the model picking some noticably high coefficient values in comparison to these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with prediction we just pass the features on unseen data\n",
    "y_predict = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted pricing of the house\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we know that y_test contains the correct prices of the house\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we want to know how far off are the predictions from the tests prices that are the actual prices.\n",
    "- By checking the scatterplot we can visually compare y_test versus the predictions you just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Residuals are the difference between the actual values y_test and the predicted values.\n",
    "\n",
    "- if residuals are normally distributed, it is a good sign and It means your model was a correct choice for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((y_test-y_predict),bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics\n",
    "\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "Comparing these metrics:\n",
    "\n",
    "- **MAE** is the easiest to understand, because it's the average error.\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are **loss functions**, because we want to minimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can calculate all these metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((metrics.mean_squared_error(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_predict)))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_predict))\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "k = X_test.shape[1]\n",
    "n = len(X_test)\n",
    "\n",
    "RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)),'.3f'))\n",
    "MSE = mean_squared_error(y_test, y_predict)\n",
    "MAE = mean_absolute_error(y_test, y_predict)\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
    "\n",
    "print('RMSE =',RMSE, '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Discussion Question 2) Interpret the abobe 3 Regression Evaluation Metrics.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('house_data.csv')\n",
    "# check for any missing values\n",
    "df.isnull().any().any()\n",
    "df = df.drop(['id','date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features to array:\n",
    "features = df.iloc[:,1:].values\n",
    "#target variable that we try to predict is price here:\n",
    "y = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0036e741702535ac7d09f989fe7660e1f54194b"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def backwardElimination(x, SL):\n",
    "    numVars = len(x[0])\n",
    "    temp = np.zeros((21613,19)).astype(int)\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        if maxVar > SL:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    temp[:,j] = x[:, j]\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    tmp_regressor = sm.OLS(y, x).fit()\n",
    "                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                    if (adjR_before >= adjR_after):\n",
    "                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n",
    "                        x_rollback = np.delete(x_rollback, j, 1)\n",
    "                        print (regressor_OLS.summary())\n",
    "                        return x_rollback\n",
    "                    else:\n",
    "                        continue\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    " \n",
    "SL = 0.05  # this is significance level Alpha\n",
    "X_opt = features[:, [0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]]\n",
    "X_Modeled = backwardElimination(X_opt, SL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Discussion Question 3) According to the above Summary Table which Variable/s are not significant after doing the Backward Elimination?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both L1 and L2 regularization aims to optimize the residual sum of squares (RSS) plus a regularization term. For ridge regression (L2), this regularization term is the **sum of the squared coefficients** times a non-negative scaling factor lambda (or alpha in our sklearn model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2-Regularization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('house_data.csv')\n",
    "# check for any missing values\n",
    "df.isnull().any().any()\n",
    "df = df.drop(['id','date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features to array:\n",
    "X = df.iloc[:,1:]\n",
    "#target variable that we try to predict is price here:\n",
    "y = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) #you could have: test_size = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "regressor_ridge = Ridge(alpha = 50)\n",
    "regressor_ridge.fit(X_train, y_train)\n",
    "print('Linear Model Coefficient (m): ', regressor_ridge.coef_)\n",
    "print('Linear Model Coefficient (b): ', regressor_ridge.intercept_)\n",
    "\n",
    "y_predict = regressor_ridge.predict( X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test, y_predict, \"o\", color = 'r')\n",
    "plt.xlim(0, 3000000)\n",
    "plt.ylim(0, 3000000)\n",
    "\n",
    "plt.xlabel(\"Model Predictions\")\n",
    "plt.ylabel(\"True Value (ground Truth)\")\n",
    "plt.title('Linear Regression Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "k = X_test.shape[1]\n",
    "n = len(X_test)\n",
    "\n",
    "RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)),'.3f'))\n",
    "MSE = mean_squared_error(y_test, y_predict)\n",
    "MAE = mean_absolute_error(y_test, y_predict)\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
    "\n",
    "print('RMSE =',RMSE, '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression (L1-Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "regressor_lasso = Lasso(alpha = 50)\n",
    "regressor_lasso.fit(X_train,y_train)\n",
    "print('Linear Model Coefficient (m): ', regressor_lasso.coef_)\n",
    "print('Linear Model Coefficient (b): ', regressor_lasso.intercept_)\n",
    "\n",
    "y_predict = regressor_lasso.predict( X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test, y_predict, \".\", color = 'r')\n",
    "plt.xlim(0, 3000000)\n",
    "plt.ylim(0, 3000000)\n",
    "\n",
    "plt.xlabel(\"Model Predictions\")\n",
    "plt.ylabel(\"True Value (ground Truth)\")\n",
    "plt.title('Linear Regression Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "RMSE = float(format(np.sqrt(mean_squared_error(y_test, y_predict)),'.3f'))\n",
    "MSE = mean_squared_error(y_test, y_predict)\n",
    "MAE = mean_absolute_error(y_test, y_predict)\n",
    "r2 = r2_score(y_test, y_predict)\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
    "\n",
    "print('RMSE =',RMSE, '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Discussion Question 4) Compare Adjusted R2 for the Multiple Linear, Backward Elimination, Ridge, and Lasso Regression Models. Which Model Performs better, Why?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
